{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM83acTl8/3rq0cblQZybZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VickkiMars/AI-ML/blob/main/Multiclass_names_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_pG_0M24pWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install names-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEaNQg5SqC4z",
        "outputId": "11e81276-f93b-4ea4-f121-b2e8db5817e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting names-dataset\n",
            "  Downloading names-dataset-3.1.0.tar.gz (58.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry (from names-dataset)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: names-dataset\n",
            "  Building wheel for names-dataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for names-dataset: filename=names_dataset-3.1.0-py3-none-any.whl size=116832758 sha256=882225b187a385111064b5b3fcffb9ae771ada02d1ba8b7aac0ed21232a1bc6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/f8/43/0c4aba87b34e971e7255a41f11dc0035c5e55b026dc3480986\n",
            "Successfully built names-dataset\n",
            "Installing collected packages: pycountry, names-dataset\n",
            "Successfully installed names-dataset-3.1.0 pycountry-24.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "crKaE_3Bo9n5"
      },
      "outputs": [],
      "source": [
        "from names_dataset import NameDataset\n",
        "import unicodedata\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import logging\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "THGRKeqy597Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.executing_eagerly())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltqwnvRRzikx",
        "outputId": "e3ca29f0-de41-47c5-f8b5-ebb3007caaf9"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.DEBUG)\n"
      ],
      "metadata": {
        "id": "Y450SeL6qyrt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_letters = string.ascii_letters + \".,;'\"\n",
        "n_letters = len(all_letters)\n",
        "LENGTH = 10\n",
        "DIR = os.getcwd()"
      ],
      "metadata": {
        "id": "a8IwUFmsqINa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn' and c in all_letters\n",
        "    )"
      ],
      "metadata": {
        "id": "v4nwuLvRqB90"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nd = NameDataset()"
      ],
      "metadata": {
        "id": "zHxygo2JqOm3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_country_names(\n",
        "        length:int,\n",
        "        code:str,\n",
        "        ) -> list:\n",
        "\n",
        "    #['GE', 'NG', 'CH', 'IT']\n",
        "    male_names = nd.get_top_names(3000, country_alpha2=code)[code]['M']\n",
        "    male_names = [name.lower() for name in male_names]\n",
        "\n",
        "    female_names = nd.get_top_names(3000, country_alpha2=code)[code]['F']\n",
        "    female_names = [name.lower() for name in female_names]\n",
        "    return male_names, female_names"
      ],
      "metadata": {
        "id": "2shousP1rIAL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filename):\n",
        "    dataset = tf.data.experimental.CsvDataset(\n",
        "        header=True,\n",
        "        filenames=filename,\n",
        "        record_defaults=[\n",
        "            tf.string\n",
        "        ]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "21fGf-JyqWq2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Chinese names\n",
        "logging.info('Retrieving Chinese names...')\n",
        "chinese_male, chinese_female = get_country_names(LENGTH, 'CH')\n",
        "logging.info('Chinese names Retrieved!')\n",
        "# Apply ASCII formatting\n",
        "logging.info('Formatting Chinese Names...')\n",
        "chinese_male, chinese_female = [unicodeToAscii(name) for name in chinese_male], [unicodeToAscii(name) for name in chinese_female]\n",
        "logging.info('Chinese Names Formatted')\n",
        "\n",
        "\n",
        "# Get Nigerian names\n",
        "logging.info('Retrieving Nigerian Names...')\n",
        "nigerian_male, nigerian_female = get_country_names(LENGTH, 'NG')\n",
        "logging.info('Nigerian Names Retrieved')\n",
        "# Apply ASCII formatting\n",
        "logging.info('Formatting Nigerian Names...')\n",
        "nigerian_male, nigerian_female = [unicodeToAscii(name) for name in nigerian_male],[unicodeToAscii(name) for name in nigerian_female]\n",
        "logging.info('Nigerian Names formatted.')\n",
        "\n",
        "logging.info('Retrieving Italian Names...')\n",
        "# Get Italian names\n",
        "italian_male, italian_female = get_country_names(LENGTH, 'IT')\n",
        "logging.info('Italian Names Retrieved.')\n",
        "# Apply ASCII formatting\n",
        "logging.info('Formatting Italian Names...')\n",
        "italian_male, italian_female = [unicodeToAscii(name) for name in italian_male], [unicodeToAscii(name) for name in italian_female]\n",
        "logging.info('Italian Names Formatted.')\n",
        "\n",
        "\n",
        "# Get German names\n",
        "logging.info('Retrieving German Names...')\n",
        "german_male, german_female = get_country_names(LENGTH, 'GE')\n",
        "logging.info('German Names retrieved')\n",
        "# Apply ASCII formatting\n",
        "logging.info('Formatting German Names...')\n",
        "german_male, german_female = [unicodeToAscii(name) for name in german_male], [unicodeToAscii(name) for name in german_female]\n",
        "logging.info('German Names Formatted')\n",
        "\n",
        "logging.info('Shuffling Names...')\n"
      ],
      "metadata": {
        "id": "l2ulNfMBqopW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male, female = [chinese_male, italian_male, german_male, nigerian_male],[chinese_female, italian_female, german_female, nigerian_female]"
      ],
      "metadata": {
        "id": "uzcX2Cm4ub9l"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_min(male, female):\n",
        "    if len(male) != len(female):\n",
        "      i, j = len(male), len(female)\n",
        "      min_ = min(i,j)\n",
        "      male, female = male[:min_], female[:min_]\n",
        "    return male,female"
      ],
      "metadata": {
        "id": "1d4qsyVDuUrZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chinese_male, chinese_female = find_min(chinese_male, chinese_female)\n",
        "italian_male, italian_female = find_min(italian_male, italian_female)\n",
        "german_male, german_female = find_min(german_male, german_female)\n",
        "nigerian_male, nigerian_female = find_min(nigerian_male, nigerian_female)"
      ],
      "metadata": {
        "id": "nNLSDWBHvcGc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_(array:list):\n",
        "  random.shuffle(array)"
      ],
      "metadata": {
        "id": "S1RnR1zttKk4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chinese = chinese_male + chinese_female\n",
        "italian = italian_male + italian_female\n",
        "german = german_male + german_female\n",
        "nigerian = nigerian_male + nigerian_female\n",
        "\n",
        "shuffle_(chinese)\n",
        "shuffle_(italian)\n",
        "shuffle_(german)\n",
        "shuffle_(nigerian)\n",
        "logging.info('Names Shuffled')"
      ],
      "metadata": {
        "id": "Agx6ej19q7YR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_ = min(len(chinese), len(italian), len(german), len(nigerian))\n",
        "chinese, italian, german, nigerian = chinese[:min_], italian[:min_], german[:min_], nigerian[:min_]"
      ],
      "metadata": {
        "id": "OFDvyUuZxpEM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL = {'Chinese':chinese,\n",
        "       'Italian':italian,\n",
        "       'German':german,\n",
        "       'Nigerian':nigerian\n",
        "       }"
      ],
      "metadata": {
        "id": "2kUb5FECrfYJ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.info('Loading into .csv format...')\n",
        "df = pd.DataFrame(ALL)\n",
        "name, dir = \"names_dataset.csv\", DIR\n",
        "df.to_csv(name)\n",
        "logging.info(f'File saved as {name} at {DIR}')"
      ],
      "metadata": {
        "id": "-k9hgoLystoX"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_all(download=False):\n",
        "    afri = ALL\n",
        "    return afri"
      ],
      "metadata": {
        "id": "n02kKEOPssEY"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(DIR+\"/\"+name)"
      ],
      "metadata": {
        "id": "hZm4PDNMzDkF"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = send_all()\n",
        "labels=  dataset.keys()\n",
        "#print(dataset['Italian'][:5])\n",
        "\n",
        "lang = [dataset[i] for i in labels]\n",
        "lang_ = [value for value in dataset.values()][0]\n",
        "#print(lang_[:10])\n",
        "lang_ = [len(i) for i in lang_]\n",
        "VOCAB_SIZE = max(lang_)\n",
        "#print(max_)"
      ],
      "metadata": {
        "id": "LUkyQY0ozzE0"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all text data and create labels\n",
        "all_text = []\n",
        "labels = []\n",
        "for lang, sentences in dataset.items():\n",
        "    all_text.extend(sentences)\n",
        "    labels.extend([lang] * len(sentences))"
      ],
      "metadata": {
        "id": "ZlvmHl4yAt1w"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle data and labels in the same order\n",
        "combined = list(zip(all_text, labels))\n",
        "random.shuffle(combined)\n",
        "all_text, labels = zip(*combined)"
      ],
      "metadata": {
        "id": "nez7RgGNAzZx"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training, validation, and testing sets (adjust ratios as needed)\n",
        "train_split = int(0.8 * len(all_text))\n",
        "val_split = int(0.9 * len(all_text))\n",
        "\n",
        "train_text = all_text[:train_split]\n",
        "train_labels = labels[:train_split]\n",
        "val_text = all_text[train_split:val_split]\n",
        "val_labels = labels[train_split:val_split]\n",
        "test_text = all_text[val_split:]\n",
        "test_labels = labels[val_split:]"
      ],
      "metadata": {
        "id": "W5nL5-FZA2XV"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Padding (using TextVectorization)\n",
        "VOCAB_SIZE = 10000  # Adjust as needed\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_text)"
      ],
      "metadata": {
        "id": "pWU7xrepz2A2"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "metadata": {
        "id": "PtElI1LATXcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to numerical sequences\n",
        "train_sequences = encoder(train_text)\n",
        "val_sequences = encoder(val_text)\n",
        "test_sequences = encoder(test_text)"
      ],
      "metadata": {
        "id": "iFXt9NWx6TPf"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a uniform length (find maximum length first)\n",
        "max_length = max([len(seq) for seq in train_sequences])\n",
        "train_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_length)\n",
        "val_sequences = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=max_length)\n",
        "test_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_length)\n",
        "\n",
        "# # Reshape the input data for the Conv1D layer\n",
        "# train_sequences = train_sequences.reshape(train_sequences.shape[0], train_sequences.shape[1], 1)\n",
        "# val_sequences = val_sequences.reshape(val_sequences.shape[0], val_sequences.shape[1], 1)\n",
        "# test_sequences = test_sequences.reshape(test_sequences.shape[0], test_sequences.shape[1], 1)"
      ],
      "metadata": {
        "id": "s0RFrzGt0WuM"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(dataset)  # Number of languages (classes)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "PLOkahVoBdq_"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (Model building steps from previous response)\n",
        "\n",
        "# 3. Model Training\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Prepare labels for training (one-hot encoding)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels_onehot = to_categorical(train_labels_encoded, num_classes=num_classes)\n",
        "val_labels_onehot = to_categorical(val_labels_encoded, num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "_dxDRugIBgRn"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model, capturing the training history\n",
        "history = model.fit(train_sequences, train_labels_onehot,\n",
        "          epochs=10,  # Adjust the number of epochs\n",
        "          validation_data=(val_sequences, val_labels_onehot))\n",
        "\n",
        "# Check if history is None\n",
        "if history is None:\n",
        "    print(\"Warning: Model training returned None. Check for early stopping or callback issues.\")\n",
        "else:\n",
        "    # Print a summary of the training results\n",
        "    print(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "T5OwrEjiBhbL",
        "outputId": "869bf17a-0276-41f4-a772-0dad3e3b2ebf"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Kernel shape must have the same length as input, but received kernel of shape (5, 64, 64) and input of shape (32, 1, 1, 64).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-cb808ed157c2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model, capturing the training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(train_sequences, train_labels_onehot, \n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Adjust the number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           validation_data=(val_sequences, val_labels_onehot))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation_utils.py\u001b[0m in \u001b[0;36mcompute_conv_output_shape\u001b[0;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mkernel_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;34m\"Kernel shape must have the same length as input, but received \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;34mf\"kernel of shape {kernel_shape} and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Kernel shape must have the same length as input, but received kernel of shape (5, 64, 64) and input of shape (32, 1, 1, 64)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGgtvqbTB6u8",
        "outputId": "a421b35c-50ee-485b-ba99-61f2c07379c2"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[   1]],\n",
              "\n",
              "       [[   1]],\n",
              "\n",
              "       [[8139]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 684]],\n",
              "\n",
              "       [[1699]],\n",
              "\n",
              "       [[8341]]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulPhvljUCvVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}