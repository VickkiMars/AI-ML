{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMQSaGA5wcYVXskDvj7kXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VickkiMars/AI-ML/blob/main/human_vs_ai_text_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUbYNltBrDsK",
        "outputId": "8761477f-05c5-4ced-c0e5-cc0b87745d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b6c69f0fd60>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/00/e7/12300c2f886b846375c78a4f32c0ae1cd20bdcf305b5ac45b8d7eceda3ec/opendatasets-0.1.22-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.5)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "oNm24p9WqP0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1RuNPHmqsdZ",
        "outputId": "5d9befb9-a9b9-438a-c2e3-e309ff9da655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: victorthamartian\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text\n",
            "Downloading ai-vs-human-text.zip to ./ai-vs-human-text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 350M/350M [00:20<00:00, 18.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TuLK6dqosEB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/ai-vs-human-text/AI_Human.csv\")"
      ],
      "metadata": {
        "id": "4olxQMhxsW1-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q -upgrade keras"
      ],
      "metadata": {
        "id": "MPZbvEFSsr1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c668f60-bf20-456e-a88e-8f055f8a58d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/548.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m216.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzYWIDhiu0an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "PnztCaUHtrab"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "R0_utQv-wCHf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/ai-vs-human-text/AI_Human.csv')\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "train.to_csv('ai_human_train.csv', index=False)\n",
        "test.to_csv('ai_human_test.csv', index=False)"
      ],
      "metadata": {
        "id": "OI6L5vI95aSg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "ai_human_train = tf.data.experimental.CsvDataset(\n",
        "    \"/content/ai_human_train.csv\",\n",
        "    [tf.string, tf.float64],\n",
        "    header=True,\n",
        ").shuffle(buffer_size=1000).batch(BATCH_SIZE)\n",
        "\n",
        "ai_human_test = tf.data.experimental.CsvDataset(\n",
        "    \"/content/ai_human_test.csv\",\n",
        "    [tf.string, tf.float64],\n",
        "    header=True,\n",
        ").shuffle(buffer_size=1000).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "CMSgMKUOt8u1"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_human_train.unbatch().batch(5).take(1).get_single_element()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sLtOTaVwgv9",
        "outputId": "ee3bea2c-7c60-4f6f-95ed-d9a8bc953a5b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"Many students have been either forced or pressured to do an extracurricular activity. They might be forced or pressured by a parent/guardian, teacher, or friend. However, that doesn't mean they should do the activity. The principal should not force students to participate in an extracurricular activity because, students might not have time to do an activity, might be forced to do an activity they don't like, or the activities might suffer.\\n\\nWhile some students have plenty of extra time to use after school, some may not. Students can usually RET their homework done either after school or during resource. Though, they don't always do it in a healthy way. Some have to stay up late to RET an assignment done either because, they forgot when it was due, waited till the last minute to do it, didn't realize how BIR it was, or have a field trip or special event they are going to. This causes the student to not RET a healthy amount of sleep, which causes them to have trouble paying attention in class. Also, some students decide to take all honors classes, which usually rive out more homework than normal classes. If the student has to deal with an after school activity on top of all of this, then they will have a very annoying schedule.\\n\\nSometimes parents force their kid to do an extracurricular activity. Usually the student either finds out that they enjoy the activity, or that they hate it. If they hate it they could ask their parents if they can stop doing the activity, but the parent could say no. Then the kid is stuck doing this activity even though they don't want to do it. The same problem can happen if a friend asks them to do an extracurricular activity with them. The kid might find out that they don't like the activity, but they don't want to upset their friend, so they continue to do the activity. If the principal says the students have to take an extracurricular activity, a few of the students might not want to do any of the possible activities. This would mean at least a few students would be forced to do an activity they don't enjoy. If students are forced to take an extracurricular activity, then they could RET stuck doing something they don't want to do.\\n\\nThere are hundreds of kids in a school. If the principal were to say that every student in the school has to do at least one extracurricular activity, it would be a mess. The after school activities that the school offers would be overflowing with students. If the school had only one team for each sport, the school would need to make a lot more. If more teams are made more equipment will be needed, which means the school would have to use more money. Also, the teams themselves would suffer, because there are bound to be kids who don't even like the sport who are only there because they have to be. This could also happen with almost any other activity in the school. If students are forced to take an extracurricular activity, the activities will suffer.\\n\\nIf students are forced to participate in an extracurricular activity, students won't have enough time to do assignments, might be forced to do an activity they dislike, and the activities the school offers will suffer. Though it may seem like a rood idea to have all the students do an extracurricular activity, it will be a pain for the students and the school.\", shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_en_uncased\", num_classes=2)"
      ],
      "metadata": {
        "id": "jxGB8HxHw5lo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wsPVdOIN1EFZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = np.asarray(classifier.predict([\"I love you\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvQr9C7WzFIL",
        "outputId": "cf97b496-f211-4711-85f6-5ac35c503499"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Softmax()\n",
        "layer(h).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znD4Q8861Lco",
        "outputId": "c9bfcefa-a9db-408e-cde7-32dd6e6b9c67"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.43362167, 0.56637836]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.evaluate(ai_human_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-TkL5UL0ivj",
        "outputId": "b26a8e10-66df-46fa-fd26-f933ed3a1418"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6091/6091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 140ms/step - loss: 0.7918 - sparse_categorical_accuracy: 0.3757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7933933138847351, 0.3729514479637146]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate"
      ],
      "metadata": {
        "id": "EKuxp85f27P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(\n",
        "    ai_human_train,\n",
        "    validation_data=ai_human_test,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puu9pg3K1iS7",
        "outputId": "e2cb8ec0-3ccf-4891-934a-d90afbbcafe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   9812/Unknown \u001b[1m4519s\u001b[0m 447ms/step - loss: 0.0592 - sparse_categorical_accuracy: 0.9790"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2Mbh-zT-8BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}